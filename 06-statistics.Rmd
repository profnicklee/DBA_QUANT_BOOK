# Introduction to Statistics {#statistics}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

Here, we begin to apply our probability concepts to statistics.

## The Law of Large Numbers

Demonstration by repeating the coin-flip simulation

Remember, this is another random process, so results will differ each time slightly

```{r}
#This is virtually copied verbatim from: https://rpubs.com/pgrosse/545948
#extended it to 10000 flips
#simulate 10000 flips of a fair coin
#create data frame for trial number and outcome of an individual coin flip
flips <- sample(c(0, 1), 10000, replace = TRUE)
flips <- matrix(flips, ncol = 1)
flips <- as.data.frame(flips)
Trial <- seq(1, 10000, 1)
Trial <- as.data.frame(Trial)
flipsim <- cbind(flips, Trial)
colnames(flipsim) <- c("Heads", "Trial")
```

```{r}
#calculate cumulative heads at the end of each trial
flipsim[,"Cum_Heads"] <- cumsum(flipsim$Heads)
flipsim <- flipsim %>% mutate(Pct_Heads = Cum_Heads/Trial)
head(flipsim)
```

```{r}
#create plot
fair_plot <- flipsim %>% ggplot(aes(y = Pct_Heads, x = Trial)) + ggtitle("Percentage of Heads \n Fair Coin") + geom_line() 

#note the below code creates the animation, but it does not look correct to me when it animates so I do not use it here. If you want to include it, move the + up to the end of the 
#last line above.
#+ geom_segment(aes(xend = 5000, yend = Pct_Heads), linetype = 2,color = "red") + geom_point(size = 2) + transition_reveal(Trial) + ylim(0,1) + coord_cartesian(clip = "off") + theme(plot.title = element_text(hjust = 0.5)) 
fair_plot
```

## The Distribution of Sample Means, and the Central Limit Theorem

First, let's read in the data set.

```{r}
CLT<-read_excel("D:/Dropbox/R_Files/Data/CLT.xlsx")
head(CLT)
```

```{r}
summary(CLT$Income)
```

Let's plot the distribution, and if we do so we will find that it is essentially uniform - every value occurs once in the population.

```{r}
ggplot(CLT, aes(x=Income))+geom_histogram(binwidth=400, colour="black", fill="white")
```

Now, what I am going to do is take a sample of 2 from that population, and take the mean

I do this using a 'combination' operation

```{r}
##code modified from: https://www.geeksforgeeks.org/calculate-combinations-and-permutations-in-r/
vec <- CLT$Income
  
# generating 1 random combination of 2 of the 
# Income values 
print ("One Random Combination of 2 of the 20 Income Values")
res1<- combinations(n= 2, r = 2, v = vec)
print (res1)


```

Let's take the mean of that:

```{r}
data1<-data.frame(res1)

head(data1)
```

```{r}
data1$MDIST <- rowMeans(data1)

head(data1)
```

So, the mean is 5800 (yes, I know there must have been a more efficient way to do that. Answers on a postcard).

So, let's do that for every possible combination of 2 values from this population.

Given there are n=20 values in the population, there are k=190 possible combinations of two values.

Note: This is not the same as bootstrapping - we are *not sampling with replacement* here. We are instead taking combinations. The thing to think about is that this is the equivalent of taking every single possible sample of 2 that you could take from this population.

Remember the ping-pong balls!

Let's do it:

```{r}
##code modified from: https://www.geeksforgeeks.org/calculate-combinations-and-permutations-in-r/
vec <- CLT$Income
  
# generating combinations of the 
# Income values taking 2 at a time
print ("Every Possible Combination of the 20 Income Values")
res<- combinations(n= 20, r = 2, v = vec)
print (res)
  
print ("Number of combinations without repetition")
print (nrow(res))


```

```{r}
data<-data.frame(res)

head(data)
```

Let's create the means again

```{r}
data$MDIST <- rowMeans(data)

head(data)
```

OK, so here is the kicker. Let's plot a histogram of these means:

```{r}
ggplot(data, aes(x=MDIST))+geom_histogram(bins=13, colour="black", fill="white")

```

Well well well!

Even though the original population was a completely uniform distribution with a mean of 9400, the distribution of all of the sample means looks quite a lot like a gaussian / normal distribution!

Let's overlay one on it as well to make the point...

```{r}
ggplot(data, aes(x=MDIST))+geom_histogram(bins = 13, aes (y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")
```

Further, let's take the mean of those means...

```{r}
summary(data$MDIST)
```

Hello! It turns out, the mean of those means is the population mean!!!

This, in a demonstration, is *the central limit theorem.*

## Rate of Change in Football Goals per Season

Here, I'm using data from <https://www.footballhistory.org/league/premier-league-statistics.html>

I hand-entered this into a spreadsheet, and calculated the additional stuff.

```{r}
EPLGOALS<-read_excel("D:/Dropbox/R_Files/Data/EPLGOALS.xlsx")
head(EPLGOALS)
```

You can see here I have calculated the standard errors from the yearly goal totals (which represent that year's underlying rate of goal occurrence), then used that to calculate the 95% Confidence Interval limits

We can use these to create a nifty chart with the error bars...drawing from the code used by Spiegelhalter in his book for Figure 9.4 available on his github (linked in the code).

```{r}
#modified from Spiegelhalter's Figure 9.4 available at:
#https://github.com/dspiegel29/ArtofStatistics/blob/master/09-4-homicide-rates-E%2BW/09-4-homicide-trends-x.Rmd

#note, the hashed-out code is not relevant to my example
#but left in in case someone else wants to use it

df<-EPLGOALS # read data to dataframe df
p <- ggplot(df, aes(x=Season, y=Goals)) # initial plot
p <- p + geom_bar(stat="identity", fill="red") # assign bar chart type

#yearLabels <- c("Apr  97-\nMar  98","Apr  00-\nMar  01","Apr  03-\nMar  #04","Apr  06-\nMar  07","Apr 09-\nMar  10","Apr  12-\nMar  13","Apr  #15-\nMar  16") # assign labels for x-axis

p <- p + geom_errorbar(aes(ymin=Lower95CI, ymax=Upper95CI), width=.1) # 95% intervals

#p <- p + scale_x_continuous(breaks=seq(1997, 2015, 3), labels =yearLabels) # attach labels and their break points

p <- p + scale_y_continuous(breaks=seq(0, 1100, 100)) # define break points for y-axis
p <- p + labs(y="Total Goals") # add y-axis label and caption
p

```

From this chart, and looking at the data itself, we can see that the 95% Intervals overlap, so it is hard to conclude that the underlying rate of goals has changed significantly year on year. Yes, even in the pandemic.

The closest we get in fact is between the 2008-2009 and 2009-10 season. Interestingly, this corresponds to when Man City were bought, and it is evident that the top teams scored a lot more that year.

This is a stringent test however, and the ONS suggest that you can also test the change by using a Z-test, which *directly tests the hypothesis* that the change is zero, using the assumption that the events are Poisson distributed (we agree) and also that when the number of events are large (generally over 20), we can use an approximation to the normal distribution.

See: <https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/compendium/focusonviolentcrimeandsexualoffences/yearendingmarch2016/homicide#statistical-interpretation-of-trends-in-homicides>

The z-test is simply explained in the linked article from the BMJ: <https://www.bmj.com/content/332/7552/1256>

It links us nicely to the next lesson, because it is aiming to test a *specific hypothesis* that the difference is zero...

If you open the data file below, you can see I have calculated the z-test results for the difference between each season, year-on-year.

```{r}
EPLZ<-read_excel("D:/Dropbox/R_Files/Data/EPLGOALSZ.xlsx")
head(EPLZ)
```

The 'Z' column is what we are interested in, and we are looking for a z-value greater than + or - 1.96 for a 95% test (analogous to the 95% intervals we've been dealing with so far).

A simple way to visualize this is to plot the z-values for each season, and include 'control lines' which represent the + or -1.96 z value, beyond which we consider there to be a significant difference

```{r}
#visualize the z values simply with control lines
U <- 1.96
L <- -1.96
p <- ggplot(EPLZ, aes(x=Season, y=Z)) + geom_point() 
p <- p+ geom_hline(aes(yintercept=U))
p <- p+ geom_hline(aes(yintercept=L))
p

```

So, we can see that the 1999-2000 season, and the 2009-10 seasons exceed our z values, making them significantly different from the seasons before.

Of course, we could do z-tests for any combination of two seasons, if we had a good reason. You can see on the ONS website they do this for different years' murder rates to make a point.
