[["index.html", "WBS DBA: Introduction to Quantitative Analysis Chapter 1 Introduction and Orientation", " WBS DBA: Introduction to Quantitative Analysis Professor Nick Lee 2024-01-15 Chapter 1 Introduction and Orientation This is an online course book, designed for students on the Warwick Business School DBA Programme, for the module ‘Introduction to Quantitative Methods’, taught by Professor Nick Lee. The module is more accurately titled ‘Learning from Data (and) Science’, and consists of a set of in-person lectures, linked with these examples and additional expositions. This book is written in Markdown, using R and the bookdown package, and published online in HTML format. A full archive of all the R Code and data is available from Professor Lee on request, once the book is completed for the year. This book is essentially a ‘live’ document, meaning that it is finalised for each year just before the module runs, but then modified for future runnings. The current year is 2024, and the book is currently in development, not final. Please enjoy the book, and learn from it. "],["intro.html", "Chapter 2 Describing the World With Data 2.1 Box Plots 2.2 Violin Plots 2.3 Describing Variables and Distributions 2.4 Transformations", " Chapter 2 Describing the World With Data Here, we will use R to demonstrate a number of important concepts about data displays and descriptions. First, we load up some data to look at. We’ll begin by looking at some COVID data. Here is some data from Johns Hopkins University as at April 26 2022 by country about cases, deaths, and mortality for COVID. I’m going to use the top 20 countries in terms of raw number of cases So, to demonstrate how we can easily change people’s views of a data set, let’s run some bar charts: That’s ugly. We could change the orientation of the labels, but it’s easier to change the orientation of the bars… That’s nice. The basic conclusion we can draw is everyone is pretty much the same in terms of survival rate, although Mexico is a bit down. Interestingly, it seems to me that the variation is more pronounced in this presentation where you look ‘down’ a line of results, than above where you look across. Anyway, if I wanted to make everyone think that COVID survival rates were really different, there’s a simple trick I could use: Wow, how do you like that! What’s different? Changing the scale of the axis is an old trick, and when used to manipulate opinion it’s a bad thing. However, there is a case to say we need to know what we want to say, before we decide how to say it. When does ‘making sure we see the right message’ move into ‘outright manipulation / misrepresentation?’. Here’s another common way this type of data is presented in media and non-scientific reports: or a variant: Technically, it’s wrong to present this data in a line chart, because it is discrete values and does not represent a trend, but it is surprisingly common - probably because it ‘looks more sciency’… Ironically, there is some logic behind using line charts with this sort of data, depending on what you are trying to get across - let’s move to excel to demonstrate… While we’re at it, we’ll also look at pie charts… Back to slide deck… 2.1 Box Plots Interesting stuff, just from the box plot. But, while box plots are nice, they can also obscure stuff. Adding the jitter helps, but there are other ways to display this type of data. 2.2 Violin Plots Let’s add jitter: Well, look at that. B has a bimodal distribution… Lets look at the violin plot to emphasize this: Neat! Another way to visualize this is with ‘ridges’, which basically compare the densities of variables together: Cool. This brings us to the idea of…. 2.3 Describing Variables and Distributions Let’s load up some more data. Here’s some GDP per head data from the World Bank: We have a number of choices as to what to do here, but the first thing we might do is plot a histogram with the data: We can change the ‘width’ of the bars with the Binwidth operator: And, we can overlay a density distribution on it: So, here we have some idea of the distribution. It’s clearly skewed quite heavily, with some outliers at the high end. Let’s summarize this with some numbers: ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 233.8 2127.5 6117.5 15162.6 18072.4 173688.2 15 ## ## GDP2020 ## --------------- ----------- ## Mean 15162.61 ## Std.Dev 22146.53 ## Min 233.84 ## Median 6117.49 ## Max 173688.19 ## N.Valid 251.00 ## Pct.Valid 94.36 Let’s chat about these for a bit… Back to slide deck 2.4 Transformations As you can see, the data is heavily skewed, or ‘squished’ towards the bottom. This makes it hard to see any patterns by eye. We can make it easier to interpret by transforming the variable. Here, using a logarithmic transform. Let’s run the box again: Nice, see how the spread is easier to interpret here? Log transforms are essentially ‘inverse exponentials’, so what they do is bring down extreme high outliers. Let’s see this in some more plots: Compare the violin of the log transformed variable with the original: In fact, we can use the log transform to revisit some of our COVID data back in EXCEL… "],["assoc_rel.html", "Chapter 3 Associations and Relationships 3.1 Correlations and Associations 3.2 Regression 3.3 Multiple Regression 3.4 Visualizing Multiple Regression", " Chapter 3 Associations and Relationships 3.1 Correlations and Associations First, let’s look at the basic concepts of correlation, using some more data from the World Happiness Report, and GDP from the World Bank, all put together by Our World in Data. Remember, you will need to point the program to where the files are on YOUR computer. Just double check the data by describing it. We’ve got 249 countries, with population, GDP per capita, and Happiness. There’s a fair bit of missing data, but that’s ok, as long as we are aware of it. ## vars n mean sd median trimmed mad ## Country* 1 249 125.00 72.02 125.00 125.00 91.92 ## Happiness 2 153 5.49 1.12 5.53 5.52 1.16 ## GDPpc 3 197 20463.88 20717.34 12655.00 17037.01 13338.95 ## Pop 4 242 59178643.60 331869505.09 5596196.00 12318073.38 8185922.38 ## min max range skew kurtosis se ## Country* 1.0 2.490000e+02 2.480000e+02 0.00 -1.21 4.56 ## Happiness 2.4 7.820000e+00 5.420000e+00 -0.26 -0.38 0.09 ## GDPpc 731.0 1.125570e+05 1.118260e+05 1.58 2.55 1476.05 ## Pop 809.0 4.663087e+09 4.663086e+09 11.65 152.44 21333379.77 Let’s run a basic scatterplot and see what happens. Interpretation, there’s seemingly an association here. As one variable increases, the other does too. Interestingly, I have chosen to put Happiness on the x-axis, which implies that the driver of GDP is happiness, whereas if you look at the media articles that are regularly written on this kind of topic, it is usually done the other way round, implying that the driver of happiness is GDP. Nevertheless, while that’s an interesting little aside which might be worth thinking about some more, it’s important that you do NOT draw any causal conclusion from this very basic scatterplot, since there are many other things which could be going on. For example, there could be a spurious relationship - what could cause both happiness and GDPpc to increase? Ideas? could it simply be ‘economic development of the country’, or ‘political stability’? Or something else? Either way, let’s flip this around to more closely resemble figures that often appear using this (and similar) data in the media: There we go, looks very like the figure on the slide from the Economist. But this association also looks kind of nonlinear to me. There are multiple ways to look at this: there may be two groups of country, low and high income, and different linear associations within those groups. It could be a nonlinear relationship? In fact, you could check this out by transforming GDPpc. Here, I might judge that a log transform might work…let’s have a go.. Check it worked… ## vars n mean sd median trimmed mad ## Country* 1 249 125.00 72.02 125.00 125.00 91.92 ## Happiness 2 153 5.49 1.12 5.53 5.52 1.16 ## GDPpc 3 197 20463.88 20717.34 12655.00 17037.01 13338.95 ## Pop 4 242 59178643.60 331869505.09 5596196.00 12318073.38 8185922.38 ## GDPpc_log 5 197 9.38 1.14 9.45 9.41 1.33 ## min max range skew kurtosis se ## Country* 1.00 2.490000e+02 2.480000e+02 0.00 -1.21 4.56 ## Happiness 2.40 7.820000e+00 5.420000e+00 -0.26 -0.38 0.09 ## GDPpc 731.00 1.125570e+05 1.118260e+05 1.58 2.55 1476.05 ## Pop 809.00 4.663087e+09 4.663086e+09 11.65 152.44 21333379.77 ## GDPpc_log 6.59 1.163000e+01 5.040000e+00 -0.25 -0.79 0.08 Let’s rerun the plot… That’s kind of cool. Now, what we could do is look at the correlations for a slightly different way of interpreting the information. Let’s pop back to the slide deck… … With that in mind, let’s take a quick look at the correlations for these variables. ## ## Pearson&#39;s product-moment correlation ## ## data: Happy$Happiness and Happy$GDPpc ## t = 13.502, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6636288 0.8092331 ## sample estimates: ## cor ## 0.745184 ## ## Pearson&#39;s product-moment correlation ## ## data: Happy$Happiness and Happy$GDPpc_log ## t = 15.916, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7287785 0.8487456 ## sample estimates: ## cor ## 0.7964703 We can see that the association is stronger (the correlation is higher) for the log GDPpc variable, although to be honest, it is not a very big difference in this data, compared to other data sets discussed in the media. Interesting in itself (note we are using 2020 data, so maybe the pandemic has something to do with it?) 3.2 Regression We can add a regression line to this scatterplot, for some extra information over the correlation I might also decide to do this with the log GDPpc to see what happens. So, this is an improvement, but not by that much in my view. That said, it’s clear that the line is a bit more accurate (the errors are lower which can be seen by the narrower shaded regions) Either way, the regression line adds a layer of information on to the correlation, which allows us to predict y from x. How so? Well, let’s add the regression equation to the chart to see: This adds an intercept, and with that plus the coefficient we have all we need to plot a straight line, which we can extrapolate to higher values of GDPpc (although remember this is the logged GDP variable) and predict what the happiness scores would be. This is obviously viable to the extent we can justify the relationship, and also within the parameters of our variables, and how confident we are that the relationship is consistent at all levels of the variables. Let’s pop back to the slide deck again for a second….. We can also use Spiegelhalter’s code to replicate his Figure 5.1 for our slide deck, and spend some time talking about it ## Family Father Mother Gender ## Length:898 Min. :62.00 Min. :58.00 Length:898 ## Class :character 1st Qu.:68.00 1st Qu.:63.00 Class :character ## Mode :character Median :69.00 Median :64.00 Mode :character ## Mean :69.23 Mean :64.08 ## 3rd Qu.:71.00 3rd Qu.:65.50 ## Max. :78.50 Max. :70.50 ## Height Kids ## Min. :56.00 Min. : 1.000 ## 1st Qu.:64.00 1st Qu.: 4.000 ## Median :66.50 Median : 6.000 ## Mean :66.76 Mean : 6.136 ## 3rd Qu.:69.70 3rd Qu.: 8.000 ## Max. :79.00 Max. :15.000 ## [1] 197 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 62.00 68.00 69.50 69.35 71.00 78.50 ## [1] 2.622034 ## [1] 197 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 58.00 62.70 64.00 63.98 65.50 70.50 ## [1] 2.355607 ## [1] 465 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 60.00 67.50 69.20 69.23 71.00 79.00 ## [1] 2.631594 ## [1] 433 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 56.00 62.50 64.00 64.11 65.50 70.50 ## [1] 2.37032 3.2.1 Figure 5.1 (page 124) Linear regression of sons’ on fathers’ heights ## ## Call: ## lm(formula = Son ~ FatherS) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3774 -1.4968 0.0181 1.6375 9.3987 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.25891 3.38663 11.30 &lt;2e-16 *** ## FatherS 0.44775 0.04894 9.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.424 on 463 degrees of freedom ## Multiple R-squared: 0.1531, Adjusted R-squared: 0.1513 ## F-statistic: 83.72 on 1 and 463 DF, p-value: &lt; 2.2e-16 ## ## Pearson&#39;s product-moment correlation ## ## data: FatherS and Son ## t = 9.1498, df = 463, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3114667 0.4656805 ## sample estimates: ## cor ## 0.3913174 Figure 5.1 Scatter of heights of 465 fathers and sons from Galton’s data (many fathers are repeated since they have multiple sons). A jitter has been added to separate the points, and the diagonal dashed line represents exact equality between son and father’s heights. The solid line is the standard ‘best-fit’ line. Each point gives rise to a ‘residual’ (dashed line), which is the size of the error were we to use the line to predict a son’s height from his father’s. 3.3 Multiple Regression Here, we will use a simple three-variable set of simulated data, which represents rates of smoking, rates of cycling, and heart disease incidence. This data is available from: https://www.scribbr.com/statistics/linear-regression-in-r/ ## vars n mean sd median trimmed mad min max range ## ...1 1 498 249.50 143.90 249.50 249.50 184.58 1.00 498.00 497.00 ## biking 2 498 37.79 21.48 35.82 37.71 27.51 1.12 74.91 73.79 ## smoking 3 498 15.44 8.29 15.81 15.47 10.86 0.53 29.95 29.42 ## heart.disease 4 498 10.17 4.57 10.39 10.18 5.42 0.55 20.45 19.90 ## skew kurtosis se ## ...1 0.00 -1.21 6.45 ## biking 0.07 -1.22 0.96 ## smoking -0.04 -1.12 0.37 ## heart.disease -0.03 -0.93 0.20 OK, so these are simple regression lines, with bivariate scatterplots. What I mean, is the effect of biking on heart disease does not take account of the effect of smoking on heart disease. What we need to do is run a model which takes account of both the predictors. Let’s pop back to the slides for a second… Now we can run the Multiple Regression Model ## ## Call: ## lm(formula = heart.disease ~ biking + smoking, data = Heart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1789 -0.4463 0.0362 0.4422 1.9331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.984658 0.080137 186.99 &lt;2e-16 *** ## biking -0.200133 0.001366 -146.53 &lt;2e-16 *** ## smoking 0.178334 0.003539 50.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.654 on 495 degrees of freedom ## Multiple R-squared: 0.9796, Adjusted R-squared: 0.9795 ## F-statistic: 1.19e+04 on 2 and 495 DF, p-value: &lt; 2.2e-16 We can interpret these results just as we did the earlier simple regression results. A 1 unit increase in biking will on average lead to a 0.2 unit decrease in heart disease A 1 unit increase in smoking will on average lead to a 0.17 unit increase in heart disease. Of course, what these ‘units’ refer to depends on what you have measured them with of course. But, the unarguable interpretation is that the effect is strong. However, because the scales of the variables are different, its not really possible to compare the sizes of the effects. So to some extent we don’t know which of the two variables has the ‘bigger’ effect here, relatively at least. To do that, we need to standardize the coeffiecients, and to do that in R, we need to create a new set of standardized data, and run the model on that, as follows: ## vars n mean sd median trimmed mad min max range skew ## ...1 1 498 0 1 0.00 0 1.28 -1.73 1.73 3.45 0.00 ## biking 2 498 0 1 -0.09 0 1.28 -1.71 1.73 3.43 0.07 ## smoking 3 498 0 1 0.05 0 1.31 -1.80 1.75 3.55 -0.04 ## heart.disease 4 498 0 1 0.05 0 1.19 -2.10 2.25 4.35 -0.03 ## kurtosis se ## ...1 -1.21 0.04 ## biking -1.22 0.04 ## smoking -1.12 0.04 ## heart.disease -0.93 0.04 ## ## Call: ## lm(formula = heart.disease ~ biking + smoking, data = std_Heart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47658 -0.09762 0.00792 0.09671 0.42283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.119e-17 6.410e-03 0.00 1 ## biking -9.403e-01 6.418e-03 -146.53 &lt;2e-16 *** ## smoking 3.234e-01 6.418e-03 50.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1431 on 495 degrees of freedom ## Multiple R-squared: 0.9796, Adjusted R-squared: 0.9795 ## F-statistic: 1.19e+04 on 2 and 495 DF, p-value: &lt; 2.2e-16 You can see the things that have changes are the ‘Estimates’, which are now standardized. In decimal, they are -0.94 for biking and 0.32 for Smoking. The correct interpretation of these standardized effects is that for every 1SD increase in biking, you expect a 0.94SD decrease in heart disease, and for every 1SD increase in smoking, you would expect a 0.32 increase in heard disease. So, we can see that actually smoking has a much higher relative effect on heart disease than smoking (although in the opposite direction). However, this assumes that both the two IVs have similar standard deviations, and distributions. Which, we didn’t check. 3.4 Visualizing Multiple Regression There are loads of different ways to visualize multiple regression. It’s not trivial, because we have more than two variables, so we can’t use the techniques we used already. Some people like to use 3D-style plots, which look cool, but are not always easy to interpret, and take quite a lot of extra work, for what I would say is not that much payoff (if any). In this case, we can use a pretty simple visualization, where we could plot the relationship of biking to heart disease at different levels of smoking. This would be a quite typical way to do things if we thought the relationship between biking and heart disease changed according to the level of smoking, in which case it would be a moderator. Here, it doesn’t really work that way, but it’s a cool visualization regardless. It does require some data prep, but not that much, and I took the basic idea from the website where I sourced the data: https://www.scribbr.com/statistics/linear-regression-in-r/ Here, we can see that the effect of smoking is really just to raise the likelihood of heart disease, however much biking you do. So, for a given person who bikes a given amount, if they smoke more they will have a higher risk than a person who bikes the same amount but smokes less. But, for a given smoker, the more they bike, the lower their risk of heart disease, to the extent that if a heavy smoker bikes enough, their actual risk of heart disease could even be lower than a non-smoker who does not bike at all. Don’t forget, this is not real data, but the point stands. "],["uncertainty.html", "Chapter 4 Understanging Uncertainty 4.1 Back to slides… 4.2 Back to slides…", " Chapter 4 Understanging Uncertainty First, let’s grab some data.Here, we will again use the simple three-variable set of simulated data, which represents rates of smoking, rates of cycling, and heart disease incidence. This data is available from: https://www.scribbr.com/statistics/linear-regression-in-r/ ## # A tibble: 6 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.8 10.9 11.8 ## 2 2 65.1 2.22 2.85 ## 3 3 1.96 17.6 17.2 ## 4 4 44.8 2.80 6.82 ## 5 5 69.4 16.0 4.06 ## 6 6 54.4 29.3 9.55 A slightly different way to look at the data can be done using the Skimr package… Table 4.1: Data summary Name Heart Number of rows 498 Number of columns 4 _______________________ Column type frequency: numeric 4 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist …1 0 1 249.50 143.90 1.00 125.25 249.50 373.75 498.00 ▇▇▇▇▇ biking 0 1 37.79 21.48 1.12 20.20 35.82 57.85 74.91 ▇▇▇▆▇ smoking 0 1 15.44 8.29 0.53 8.28 15.81 22.57 29.95 ▆▆▇▆▆ heart.disease 0 1 10.17 4.57 0.55 6.51 10.39 13.72 20.45 ▃▇▇▇▂ Let’s calculate some simple summary statistics from this data set to build on. For example, what is the mean and median for ‘smoking’? ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.5259 8.2798 15.8146 15.4350 22.5689 29.9467 There we go. Let’s start to build a table using these values, in the slide deck… 4.1 Back to slides… Now, we know this is simulated data, but let’s imagine that it was actually obtained by an organization like Gallup, or the Office for National Statistics in the UK, using a survey. We can presume the study was done well, and thus it is based on a true random sampling method, and that we assume the study population matches whatever target population we have in mind (remember the ‘inference gaps’). What we really want to know is, how close are these statistics (i.e. the mean and median) to the true population values that we would have found if we could survey the entire target population? Let’s demonstrate this using an example. First, let’s assume that this sample of 498 people actually now is the population we are interested in. So, we can actually sample from this population and see what happens. First, let’s present the distribution for the entire ‘population’ of 498. Now, let’s actually take a sample of 10 random cases from that population of 498, and look at the relevant statistics and distribution:: ## # A tibble: 10 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 106 22.5 7.70 11.7 ## 2 135 70.3 26.2 5.24 ## 3 121 65.6 14.1 4.04 ## 4 362 49.9 3.86 5.33 ## 5 462 34.8 29.8 13.4 ## 6 286 25.7 23.3 13.6 ## 7 274 55.4 17.2 7.23 ## 8 304 40.6 2.78 6.57 ## 9 62 11.5 6.74 14.6 ## 10 365 59.9 22.9 7.32 ## [1] 15.65524 ## [1] 15.45941 We can do the same for successively larger samples, say 50, and 200: ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 326 44.6 17.2 8.31 ## 2 25 30.4 17.0 12.4 ## 3 483 36.1 13.8 9.73 ## 4 186 45.6 20.6 9.80 ## 5 354 2.98 3.84 15.2 ## 6 3 1.96 17.6 17.2 ## 7 219 71.0 15.3 4.08 ## 8 162 22.3 23.2 15.9 ## 9 81 64.4 10.5 4.57 ## 10 70 22.9 24.6 16.2 ## # ℹ 40 more rows ## [1] 15.07986 ## [1] 14.53667 ## # A tibble: 200 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 90 34.7 8.64 8.33 ## 2 389 21.5 4.76 10.4 ## 3 288 45.5 25.8 10.9 ## 4 404 24.8 6.51 11.6 ## 5 371 26.3 26.7 15.1 ## 6 466 60.6 23.8 5.98 ## 7 59 57.2 12.8 5.31 ## 8 240 21.6 9.35 14.0 ## 9 239 61.4 22.9 6.84 ## 10 285 61.9 26.3 6.86 ## # ℹ 190 more rows ## [1] 15.34373 ## [1] 15.34373 As you can see, the distributions of the smaller samples are more peaky and bumpy, because they are very sensitve to individual data points. As the sample gets larger, it starts to look more like the population right? We can build a table now in the slides of the sample statistics (mean and median) showing that in general, as we get closer to the population size, the statistics get closer too. 4.2 Back to slides… "],["bootstrap.html", "Chapter 5 Introduction to Bootstrapping 5.1 Bootstrapping our Previous Samples 5.2 Bootstrapping Other Stuff… 5.3 T-Tests for Means", " Chapter 5 Introduction to Bootstrapping First, we will demonstrate the basic principle. Let’s use our last sample of 50 from the above example, first we will remind ourselves of its properties and distribution: ## [1] 15.07986 ## [1] 14.53667 Ok lovely. Now, what we do is draw another random sample of 50 from this 50, but each time we draw a data point, we replace it back, so we are always drawing our sample from the full 50. This is called sampling with replacement. In this way, the new sample can only contain values which were in the original sample, but will contain different numbers of those values, so the distribution will be different, and the statistics may also be different. Let’s do this, and take the mean and median of the sample: ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 205 24.0 7.61 11.6 ## 2 25 30.4 17.0 12.4 ## 3 433 3.22 9.18 16.5 ## 4 56 28.6 1.15 10.1 ## 5 19 39.7 12.7 9.75 ## 6 70 22.9 24.6 16.2 ## 7 19 39.7 12.7 9.75 ## 8 391 40.0 14.5 9.51 ## 9 343 19.9 19.1 14.5 ## 10 483 36.1 13.8 9.73 ## # ℹ 40 more rows ## [1] 14.14764 ## [1] 13.54231 Marvellous! Now, let’s do this twice more… ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 194 13.2 9.99 14.5 ## 2 433 3.22 9.18 16.5 ## 3 326 44.6 17.2 8.31 ## 4 391 40.0 14.5 9.51 ## 5 110 68.2 2.19 1.97 ## 6 427 67.5 14.6 4.70 ## 7 161 30.5 2.53 8.94 ## 8 408 50.8 20.0 7.93 ## 9 206 44.0 6.58 8.50 ## 10 391 40.0 14.5 9.51 ## # ℹ 40 more rows ## [1] 14.79793 ## [1] 14.50702 ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 131 64.9 10.9 3.77 ## 2 25 30.4 17.0 12.4 ## 3 29 48.6 10.4 6.66 ## 4 468 56.2 19.2 6.51 ## 5 205 24.0 7.61 11.6 ## 6 26 53.0 27.7 9.09 ## 7 439 63.8 9.91 3.40 ## 8 26 53.0 27.7 9.09 ## 9 205 24.0 7.61 11.6 ## 10 446 25.3 18.1 13.2 ## # ℹ 40 more rows ## [1] 14.63547 ## [1] 14.81765 We can build a table in the slides of these mean and median values So, this is the basic principle of bootstrapping. We sample with replacement from our original sample, many many times. We did 3 here manually, but we generally use a routine to do this thousands of times. back to slide deck 5.1 Bootstrapping our Previous Samples So, what we can do now, is run 1000 bootstrap replications of each of our varying-sized subsamples of the smoking data, to see what might happen: First, the 10: ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sub.10, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 15.45941 0.1713939 3.01156 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% ( 9.39, 21.19 ) ## Calculations and Intervals on Original Scale Let’s do it for the other two subsamples of n=50, and n=200 ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sub.50, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 14.53667 0.04963236 1.18755 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (12.16, 16.81 ) ## Calculations and Intervals on Original Scale ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sub.200, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 14.89638 -0.02360832 0.5908945 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (13.76, 16.08 ) ## Calculations and Intervals on Original Scale And, finally, let’s bootstrap our original full sample of 498: ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Heart, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 15.43503 0.001437866 0.359051 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (14.73, 16.14 ) ## Calculations and Intervals on Original Scale This is a very nice set of results, which can tell us many interesting things… 5.1.1 so let’s go back to the slides….. 5.2 Bootstrapping Other Stuff… We have so far only bootstrapped the mean. However, the basic principle can be applied to virtually any statistical estimate. So, we can revisit some of our prior models, and use the bootstrap to quantify the uncertainty in the estimates we previously accepted without really questioning them. And, we can introduce a new analysis tool to explore the difference between groups, and use the bootstrap with that too. 5.2.1 Correlations First, let’s revisit our recent correlation analysis of Happiness and GDP per capita. ## # A tibble: 6 × 4 ## Country Happiness GDPpc Pop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2.4 1971 38972236 ## 2 Albania 5.2 13192 2866850 ## 3 Algeria 5.12 10735 43451668 ## 4 American Samoa NA NA 46216 ## 5 Andorra NA NA 77723 ## 6 Angola NA 6110 33428490 Table 5.1: Data summary Name Happy Number of rows 249 Number of columns 4 _______________________ Column type frequency: character 1 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Country 0 1 4 32 0 249 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Happiness 96 0.61 5.49 1.12 2.4 4.67 5.53 6.26 7.820000e+00 ▁▅▇▇▃ GDPpc 52 0.79 20463.88 20717.34 731.0 4917.00 12655.00 30100.00 1.125570e+05 ▇▂▁▁▁ Pop 7 0.97 59178643.60 331869505.09 809.0 415292.50 5596196.00 24205600.00 4.663087e+09 ▇▁▁▁▁ If we run the same analysis as last time, we’ll get the same results: Correlation R = 0.75 Now, let’s take uncertainty into account, by bootstrapping that correlation and creating some confidence intervals, using a cool package called Infer, which makes this almost frighteningly easy: ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.689 0.811 ## Response: Happiness (numeric) ## Explanatory: GDPpc (numeric) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 0.745 So, you can see the correlation is 0.75 with a 95% confidence interval of 0.69 - 0.81 Now, let’s extend this to the multiple regression case we have previously used, examining the relationships between smoking, biking, and heart disease. Lets load up the data if needed ## # A tibble: 6 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.8 10.9 11.8 ## 2 2 65.1 2.22 2.85 ## 3 3 1.96 17.6 17.2 ## 4 4 44.8 2.80 6.82 ## 5 5 69.4 16.0 4.06 ## 6 6 54.4 29.3 9.55 Table 5.2: Data summary Name Heart Number of rows 498 Number of columns 4 _______________________ Column type frequency: numeric 4 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist …1 0 1 249.50 143.90 1.00 125.25 249.50 373.75 498.00 ▇▇▇▇▇ biking 0 1 37.79 21.48 1.12 20.20 35.82 57.85 74.91 ▇▇▇▆▇ smoking 0 1 15.44 8.29 0.53 8.28 15.81 22.57 29.95 ▆▆▇▆▆ heart.disease 0 1 10.17 4.57 0.55 6.51 10.39 13.72 20.45 ▃▇▇▇▂ Here, we need to calculate multiple intervals as we have multiple estimates, so it’s a bit more complicated, but still fairly intuitive using Infer: ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 intercept 15.0 ## 2 smoking 0.178 ## 3 biking -0.200 ## # A tibble: 3 × 3 ## term lower_ci upper_ci ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biking -0.203 -0.197 ## 2 intercept 14.8 15.1 ## 3 smoking 0.171 0.186 5.2.2 Back to the slides… 5.3 T-Tests for Means We can also use bootstrapping as an entry point to a new analysis situation, where we are comparing two groups. This could be for example in a classic experimental context; treatment and control. So, let’s load up our Ed Sheeran study data: ## # A tibble: 6 × 3 ## ID GROUP ANGER ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 4 ## 2 2 2 5 ## 3 3 1 2 ## 4 4 2 3 ## 5 5 2 4 ## 6 6 1 2 Table 5.3: Data summary Name ED_IND Number of rows 30 Number of columns 3 _______________________ Column type frequency: numeric 3 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1 15.50 8.80 1 8.25 15.5 22.75 30 ▇▇▇▇▇ GROUP 0 1 1.50 0.51 1 1.00 1.5 2.00 2 ▇▁▁▁▇ ANGER 0 1 3.33 1.21 1 2.00 3.0 4.00 5 ▁▇▇▆▇ We need to tell R that GROUP is a factor variable not a numeric one: Check it worked: Table 5.4: Data summary Name ED_IND Number of rows 30 Number of columns 3 _______________________ Column type frequency: factor 1 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts GROUP 0 1 FALSE 2 1: 15, 2: 15 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1 15.50 8.80 1 8.25 15.5 22.75 30 ▇▇▇▇▇ ANGER 0 1 3.33 1.21 1 2.00 3.0 4.00 5 ▁▇▇▆▇ Let’s run an independent samples T-Test with bootstrapped confidence interval using Infer. We use an independent samples test, as the theory is these two groups are sampled from independent populations (those who listened to Ed Sheeran, and those who did not) and what we are doing is trying to work out whether there is any difference in anger between them… ## # A tibble: 2 × 2 ## GROUP name ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 2.8 ## 2 2 3.87 ## Response: ANGER (numeric) ## Explanatory: GROUP (factor) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 -2.65 ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -5.49 -0.575 Remember: Group 1 is the control, and Group 2 listened to Ed Sheeran Cool so it seems that Group 2 displayed more anger. The Confidence interval for the t-statistic does not contain 0, so it supports the idea that there is a difference here. It is quite wide though - because of our small sample size. OK, so let’s use a different design, using a paired samples t-test. 5.3.1 Back to the slides… The same basic process is needed, but with some modifications because of the type of comparison we are doing. First, we read the data in as normal, and make sure it is treated as a data frame. ## ID ANG_T1 ANG_T2 ## 1 1 1 4 ## 2 2 2 5 ## 3 3 3 2 ## 4 4 4 3 ## 5 5 2 4 ## 6 6 1 2 Now, for this bootstrap purpose we actually need to to create a new column which is the difference between the two measurements (here, T1 and T2). Then, we bootstrap a one-sample t-test with this new column Let us do this: Create new difference column: ## ID ANG_T1 ANG_T2 DIF ## 1 1 1 4 3 ## 2 2 2 5 3 ## 3 3 3 2 -1 ## 4 4 4 3 -1 ## 5 5 2 4 2 ## 6 6 1 2 1 Method 1: we use Infer to bootstrap a CI for the mean of the difference variable, to see whether it includes zero: ## Response: DIF (numeric) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 1.33 ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9 1.73 ## [1] 2 ## [1] 3.333333 Method 2: We use a variant of this code to run a 1-sample t-test on the difference variable, testing whether it is different from zero. To do this we need to add some code specifying a comparison with zero: ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0 ## Response: DIF (numeric) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 1.33 ## [1] 2 ## [1] 3.333333 Marvelous. We can see that, with both ways of looking at this test, the results suggest that after listening to Ed Sheeran, our sample on average reported more anger 1: Because the 95% confidence interval does not include 0, I am confident in saying that there is some effect going on here. 2: Because the bootstrapped probability (as expressed by the p-value) of observing the mean difference in anger between T1 and T2 of 1.33 is very low (0 in our results, but the true value will not be exactly zero), I am confident in saying there is an effect here. Not surprised… "],["probability.html", "Chapter 6 Introduction to Probability 6.1 Simulating Coin Flips 6.2 Premier League Goals and the Poisson Distribution", " Chapter 6 Introduction to Probability In this lesson, we will learn about the classical theory of probability, through a set of worked examples in R. 6.1 Simulating Coin Flips The first thing we’ll do is simulate some fair coin flips (we’ll come back to this later as well) Note: This is virtually copied verbatim from: https://rpubs.com/pgrosse/545948 Remember, this is another random process, so results will differ each time slightly ## Heads Trial Cum_Heads Pct_Heads ## 1 1 1 1 1.0000000 ## 2 1 2 2 1.0000000 ## 3 0 3 2 0.6666667 ## 4 0 4 2 0.5000000 ## 5 1 5 3 0.6000000 ## 6 0 6 3 0.5000000 Below, we’ll plot the results of this simulation for ease of interpretation 6.2 Premier League Goals and the Poisson Distribution Note, much of the idea and code for this section comes from: https://bookdown.org/theqdata/honors_thesis/goal-scoring-and-the-poisson-process.html The data comes from: https://www.football-data.co.uk/englandm.php and is the EPL results for 21-22 Season However, I admit that for the sake of time, I created in Excel a new table of Total Goals (TOTG) rather than here in R. ## # A tibble: 6 × 107 ## Div Date Time HomeTeam AwayTeam FTHG FTAG ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E0 2021-08-13 00:00:00 1899-12-31 20:00:00 Brentford Arsenal 2 0 ## 2 E0 2021-08-14 00:00:00 1899-12-31 12:30:00 Man United Leeds 5 1 ## 3 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Burnley Brighton 1 2 ## 4 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Chelsea Crystal … 3 0 ## 5 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Everton Southamp… 3 1 ## 6 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Leicester Wolves 1 0 ## # ℹ 100 more variables: TOTG &lt;dbl&gt;, FTR &lt;chr&gt;, HTHG &lt;dbl&gt;, HTAG &lt;dbl&gt;, ## # HTR &lt;chr&gt;, Referee &lt;chr&gt;, HS &lt;dbl&gt;, AS &lt;dbl&gt;, HST &lt;dbl&gt;, AST &lt;dbl&gt;, ## # HF &lt;dbl&gt;, AF &lt;dbl&gt;, HC &lt;dbl&gt;, AC &lt;dbl&gt;, HY &lt;dbl&gt;, AY &lt;dbl&gt;, HR &lt;dbl&gt;, ## # AR &lt;dbl&gt;, B365H &lt;dbl&gt;, B365D &lt;dbl&gt;, B365A &lt;dbl&gt;, BWH &lt;dbl&gt;, BWD &lt;dbl&gt;, ## # BWA &lt;dbl&gt;, IWH &lt;dbl&gt;, IWD &lt;dbl&gt;, IWA &lt;dbl&gt;, PSH &lt;dbl&gt;, PSD &lt;dbl&gt;, ## # PSA &lt;dbl&gt;, WHH &lt;dbl&gt;, WHD &lt;dbl&gt;, WHA &lt;dbl&gt;, VCH &lt;dbl&gt;, VCD &lt;dbl&gt;, ## # VCA &lt;dbl&gt;, MaxH &lt;dbl&gt;, MaxD &lt;dbl&gt;, MaxA &lt;dbl&gt;, AvgH &lt;dbl&gt;, AvgD &lt;dbl&gt;, … ## Length Class Mode ## 380 character character The above is a quick way of calculating / checking the total number of games if you don’t know already. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 3.000 2.818 4.000 9.000 ## [1] 1071 First, let’s plot the distribution of goals. If you’ve ever seen a Poisson distribution, you’ll recognise this! So, this looks fairly Poisson-y, but it’s always a good idea to check this in a bit more depth. In fact, there is a formal test of this, but we don’t need to go to that depth here. First, let’s create a table of all the matches with different numbers of goals. ## # A tibble: 9 × 2 ## TOTG ActualMatches ## &lt;dbl&gt; &lt;int&gt; ## 1 0 22 ## 2 1 65 ## 3 2 88 ## 4 3 81 ## 5 4 60 ## 6 5 43 ## 7 6 17 ## 8 7 3 ## 9 9 1 Let’s pull some stats of the Total Goals variable. ## min Q1 median Q3 max mean sd n missing ## 0 2 3 4 9 2.818421 1.626359 380 0 Below is some code which will help us to create the distributions and also check them. Importantly, because the Poisson distribution is described only by its mean, the first and most basic check we can do is whether the mean and the variance of the variable are the same (or at least very close) ## [1] 2.818421 ## [1] 2.645042 This is not exactly the same, but good enough to be going on with. So, we are happy that we can try to use the Poisson distribution to describe the number of total goals scored in a premier league game. The first thing I am going to do is build a figure which compares the actual numbers of goals scored in games with the predicted numbers which would be scored if the goals scored were a perfect Poisson distribution. This is really just for illustration purposes, not to actually answer our core question, which we will come to… Ideally, we would build the Poisson probabilities for 0-9 goals (which is our maximum number scored in 21-22). However, if you do this, you will find that the two tables (actual and predicted goals) will have different numbers of rows. This is because there were no games with 8 goals scored in 21-22, but one with 9. This is a bit annoying, but not a big issue. What we do is build a Poisson probability distribution for 0-8 goals, and treat the final probability as that for ‘8 or more’ goals, for the purposes of drawing our figure. This isn’t strictly correct, because we have not collected up our actual goal data into that category, but we could do that if we wanted to. Anyway, later, we’ll break this down properly to answer the question of whether we should predict a game with 10 goals in the 22-23 season, but for our showy graph we don’t really need to do it. ## PoisProb ## 1 0.059700132 ## 2 0.168260108 ## 3 0.237113915 ## 4 0.222762284 ## 5 0.156959477 ## 6 0.088475579 ## 7 0.041560239 ## 8 0.016733465 ## 9 0.005895244 So, this allows us to make some predictions of what we could expect. Remembering again that there are 380 games in a season, we can see that there is a 0.16% chance of seeing 4 goals in a game, which equates to 380 x 0.16 = 60.8 games we would expect to see in a season with 4 goals. We can check this out by creating a new table comparing actual with predicted values… ## # A tibble: 9 × 3 ## TOTG ActualMatches ExpectedMatches ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 22 23 ## 2 1 65 64 ## 3 2 88 90 ## 4 3 81 85 ## 5 4 60 60 ## 6 5 43 34 ## 7 6 17 16 ## 8 7 3 6 ## 9 9 1 2 Remember, as you can see above, the row for ‘8’ is missing, and it goes straight to 9. So, to repeat, just take (for simplicity’s sake here) the final row of Expected Matches as meaning ‘2 games with 8 or more goals’ Anyway, it’s pretty scarily close! E.g. in 21-22 there were 60 matches with 4 goals, and that is identical to the predicted amount (the difference with the predicted 60.8 from the probability distribution is likely a rounding error). Let’s plot this for the payoff: So, the above was a bit of show, but what we want is the ‘go’. In other words, should we place a bet that there will be a game in the 22-23 season with 10 goals? Remember, there were 380 games in 21-22, none of which contained 10 goals. So, for ease of thinking about this, let’s make the assumption that the PL started in 2021, and there were no games before to count (in reality, we would go back to the last time 10 goals were scored and count from there, but let’s go with this in the first instance, and we’ll expand later on, promise). So, first, we need to calculate a new set of probabilities out to 10 goals. ## PoisProb ## 1 0.0597001317 ## 2 0.1682601080 ## 3 0.2371139154 ## 4 0.2227622837 ## 5 0.1569594775 ## 6 0.0884755792 ## 7 0.0415602392 ## 8 0.0167334647 ## 9 0.0058952437 ## 10 0.0018461421 ## 11 0.0005203206 ## 12 0.0001333166 So, the number we want is the second-last probability: 0.00052. This is because the table starts from the probability of 0 goals. This means we can expect a game with 10 goals to happen every 1923 games Calculate this by dividing 1 by the probability. So, given there are 380 games per season, you would expect a game with 10 goals to happen once every 5 seasons. So, if I started counting from the 21-22 season, the answer is NO, I would not bet on there being a game with 10 goals in the 22-23 Premier League season. But, let’s add some further context, as of November 2022, there have been 5 games in the history of the Premier League where 10 goals have been scored (and 1 with 11). The Premier League has been going since 1992, and so far in Nov 22 there have been…30 seasons. So, we are probably due one. AND, the last game with 10 goals was in 2013 (Man Utd 5, West Brom 5) So, I reckon we are definitely-maybe-probably due one. I might have a flutter after all… By the way, as of the start of the 22-23 season, there had been 21 matches with 9 goals. We would expect given our Poisson distribution that a game with 9 goals should happen every 1.4 seasons, meaning over 30 seasons we would expect…. 21. 2023 Update So, the above example is totally unchanged from when I originally coded it in 2022. The million dollar question: was there a game with 10 goals in the 22-23 season…? The answer is no, there wasn’t. But there were two with 9! Luckily I am not a betting man. I do think we are due one, so this season is probably a good bet… Unless of course you think something has fundamentally changed about the premier league in recent years, changing the probability of goals (and thus suggesting recent seasons are drawn from a Poission distribution with different characteristics than past ones), which is in fact an interesting question… "],["statistics.html", "Chapter 7 Introduction to Statistics 7.1 The Law of Large Numbers 7.2 The Distribution of Sample Means, and the Central Limit Theorem 7.3 Rate of Change in Football Goals per Season", " Chapter 7 Introduction to Statistics Here, we begin to apply our probability concepts to statistics. 7.1 The Law of Large Numbers Demonstration by repeating the coin-flip simulation Remember, this is another random process, so results will differ each time slightly ## Heads Trial Cum_Heads Pct_Heads ## 1 0 1 0 0.0000000 ## 2 0 2 0 0.0000000 ## 3 1 3 1 0.3333333 ## 4 0 4 1 0.2500000 ## 5 1 5 2 0.4000000 ## 6 0 6 2 0.3333333 7.2 The Distribution of Sample Means, and the Central Limit Theorem First, let’s read in the data set. ## # A tibble: 6 × 2 ## Person Income ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 5600 ## 2 B 6000 ## 3 C 6400 ## 4 D 6800 ## 5 E 7200 ## 6 F 7600 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5600 7500 9400 9400 11300 13200 Let’s plot the distribution, and if we do so we will find that it is essentially uniform - every value occurs once in the population. Now, what I am going to do is take a sample of 2 from that population, and take the mean I do this using a ‘combination’ operation ## [1] &quot;One Random Combination of 2 of the 20 Income Values&quot; ## [,1] [,2] ## [1,] 5600 6000 Let’s take the mean of that: ## X1 X2 ## 1 5600 6000 ## X1 X2 MDIST ## 1 5600 6000 5800 So, the mean is 5800 (yes, I know there must have been a more efficient way to do that. Answers on a postcard). So, let’s do that for every possible combination of 2 values from this population. Given there are n=20 values in the population, there are k=190 possible combinations of two values. Note: This is not the same as bootstrapping - we are not sampling with replacement here. We are instead taking combinations. The thing to think about is that this is the equivalent of taking every single possible sample of 2 that you could take from this population. Remember the ping-pong balls! Let’s do it: ## [1] &quot;Every Possible Combination of the 20 Income Values&quot; ## [,1] [,2] ## [1,] 5600 6000 ## [2,] 5600 6400 ## [3,] 5600 6800 ## [4,] 5600 7200 ## [5,] 5600 7600 ## [6,] 5600 8000 ## [7,] 5600 8400 ## [8,] 5600 8800 ## [9,] 5600 9200 ## [10,] 5600 9600 ## [11,] 5600 10000 ## [12,] 5600 10400 ## [13,] 5600 10800 ## [14,] 5600 11200 ## [15,] 5600 11600 ## [16,] 5600 12000 ## [17,] 5600 12400 ## [18,] 5600 12800 ## [19,] 5600 13200 ## [20,] 6000 6400 ## [21,] 6000 6800 ## [22,] 6000 7200 ## [23,] 6000 7600 ## [24,] 6000 8000 ## [25,] 6000 8400 ## [26,] 6000 8800 ## [27,] 6000 9200 ## [28,] 6000 9600 ## [29,] 6000 10000 ## [30,] 6000 10400 ## [31,] 6000 10800 ## [32,] 6000 11200 ## [33,] 6000 11600 ## [34,] 6000 12000 ## [35,] 6000 12400 ## [36,] 6000 12800 ## [37,] 6000 13200 ## [38,] 6400 6800 ## [39,] 6400 7200 ## [40,] 6400 7600 ## [41,] 6400 8000 ## [42,] 6400 8400 ## [43,] 6400 8800 ## [44,] 6400 9200 ## [45,] 6400 9600 ## [46,] 6400 10000 ## [47,] 6400 10400 ## [48,] 6400 10800 ## [49,] 6400 11200 ## [50,] 6400 11600 ## [51,] 6400 12000 ## [52,] 6400 12400 ## [53,] 6400 12800 ## [54,] 6400 13200 ## [55,] 6800 7200 ## [56,] 6800 7600 ## [57,] 6800 8000 ## [58,] 6800 8400 ## [59,] 6800 8800 ## [60,] 6800 9200 ## [61,] 6800 9600 ## [62,] 6800 10000 ## [63,] 6800 10400 ## [64,] 6800 10800 ## [65,] 6800 11200 ## [66,] 6800 11600 ## [67,] 6800 12000 ## [68,] 6800 12400 ## [69,] 6800 12800 ## [70,] 6800 13200 ## [71,] 7200 7600 ## [72,] 7200 8000 ## [73,] 7200 8400 ## [74,] 7200 8800 ## [75,] 7200 9200 ## [76,] 7200 9600 ## [77,] 7200 10000 ## [78,] 7200 10400 ## [79,] 7200 10800 ## [80,] 7200 11200 ## [81,] 7200 11600 ## [82,] 7200 12000 ## [83,] 7200 12400 ## [84,] 7200 12800 ## [85,] 7200 13200 ## [86,] 7600 8000 ## [87,] 7600 8400 ## [88,] 7600 8800 ## [89,] 7600 9200 ## [90,] 7600 9600 ## [91,] 7600 10000 ## [92,] 7600 10400 ## [93,] 7600 10800 ## [94,] 7600 11200 ## [95,] 7600 11600 ## [96,] 7600 12000 ## [97,] 7600 12400 ## [98,] 7600 12800 ## [99,] 7600 13200 ## [100,] 8000 8400 ## [101,] 8000 8800 ## [102,] 8000 9200 ## [103,] 8000 9600 ## [104,] 8000 10000 ## [105,] 8000 10400 ## [106,] 8000 10800 ## [107,] 8000 11200 ## [108,] 8000 11600 ## [109,] 8000 12000 ## [110,] 8000 12400 ## [111,] 8000 12800 ## [112,] 8000 13200 ## [113,] 8400 8800 ## [114,] 8400 9200 ## [115,] 8400 9600 ## [116,] 8400 10000 ## [117,] 8400 10400 ## [118,] 8400 10800 ## [119,] 8400 11200 ## [120,] 8400 11600 ## [121,] 8400 12000 ## [122,] 8400 12400 ## [123,] 8400 12800 ## [124,] 8400 13200 ## [125,] 8800 9200 ## [126,] 8800 9600 ## [127,] 8800 10000 ## [128,] 8800 10400 ## [129,] 8800 10800 ## [130,] 8800 11200 ## [131,] 8800 11600 ## [132,] 8800 12000 ## [133,] 8800 12400 ## [134,] 8800 12800 ## [135,] 8800 13200 ## [136,] 9200 9600 ## [137,] 9200 10000 ## [138,] 9200 10400 ## [139,] 9200 10800 ## [140,] 9200 11200 ## [141,] 9200 11600 ## [142,] 9200 12000 ## [143,] 9200 12400 ## [144,] 9200 12800 ## [145,] 9200 13200 ## [146,] 9600 10000 ## [147,] 9600 10400 ## [148,] 9600 10800 ## [149,] 9600 11200 ## [150,] 9600 11600 ## [151,] 9600 12000 ## [152,] 9600 12400 ## [153,] 9600 12800 ## [154,] 9600 13200 ## [155,] 10000 10400 ## [156,] 10000 10800 ## [157,] 10000 11200 ## [158,] 10000 11600 ## [159,] 10000 12000 ## [160,] 10000 12400 ## [161,] 10000 12800 ## [162,] 10000 13200 ## [163,] 10400 10800 ## [164,] 10400 11200 ## [165,] 10400 11600 ## [166,] 10400 12000 ## [167,] 10400 12400 ## [168,] 10400 12800 ## [169,] 10400 13200 ## [170,] 10800 11200 ## [171,] 10800 11600 ## [172,] 10800 12000 ## [173,] 10800 12400 ## [174,] 10800 12800 ## [175,] 10800 13200 ## [176,] 11200 11600 ## [177,] 11200 12000 ## [178,] 11200 12400 ## [179,] 11200 12800 ## [180,] 11200 13200 ## [181,] 11600 12000 ## [182,] 11600 12400 ## [183,] 11600 12800 ## [184,] 11600 13200 ## [185,] 12000 12400 ## [186,] 12000 12800 ## [187,] 12000 13200 ## [188,] 12400 12800 ## [189,] 12400 13200 ## [190,] 12800 13200 ## [1] &quot;Number of combinations without repetition&quot; ## [1] 190 ## X1 X2 ## 1 5600 6000 ## 2 5600 6400 ## 3 5600 6800 ## 4 5600 7200 ## 5 5600 7600 ## 6 5600 8000 Let’s create the means again ## X1 X2 MDIST ## 1 5600 6000 5800 ## 2 5600 6400 6000 ## 3 5600 6800 6200 ## 4 5600 7200 6400 ## 5 5600 7600 6600 ## 6 5600 8000 6800 OK, so here is the kicker. Let’s plot a histogram of these means: Well well well! Even though the original population was a completely uniform distribution with a mean of 9400, the distribution of all of the sample means looks quite a lot like a gaussian / normal distribution! Let’s overlay one on it as well to make the point… Further, let’s take the mean of those means… ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5800 8200 9400 9400 10600 13000 Hello! It turns out, the mean of those means is the population mean!!! This, in a demonstration, is the central limit theorem. 7.3 Rate of Change in Football Goals per Season Here, I’m using data from https://www.footballhistory.org/league/premier-league-statistics.html I hand-entered this into a spreadsheet, and calculated the additional stuff. ## # A tibble: 6 × 7 ## Season Games Goals GPG SE Lower95CI Upper95CI ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-96 380 988 2.6 31.4 926. 1050. ## 2 1996-97 380 970 2.55 31.1 909. 1031. ## 3 1997-98 380 1019 2.68 31.9 956. 1082. ## 4 1998-99 380 959 2.52 31.0 898. 1020. ## 5 1999-00 380 1060 2.79 32.6 996. 1124. ## 6 2000-01 380 992 2.61 31.5 930. 1054. You can see here I have calculated the standard errors from the yearly goal totals (which represent that year’s underlying rate of goal occurrence), then used that to calculate the 95% Confidence Interval limits We can use these to create a nifty chart with the error bars…drawing from the code used by Spiegelhalter in his book for Figure 9.4 available on his github (linked in the code). From this chart, and looking at the data itself, we can see that the 95% Intervals overlap, so it is hard to conclude that the underlying rate of goals has changed significantly year on year. Yes, even in the pandemic. The closest we get in fact is between the 2008-2009 and 2009-10 season. Interestingly, this corresponds to when Man City were bought, and it is evident that the top teams scored a lot more that year. This is a stringent test however, and the ONS suggest that you can also test the change by using a Z-test, which directly tests the hypothesis that the change is zero, using the assumption that the events are Poisson distributed (we agree) and also that when the number of events are large (generally over 20), we can use an approximation to the normal distribution. See: https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/compendium/focusonviolentcrimeandsexualoffences/yearendingmarch2016/homicide#statistical-interpretation-of-trends-in-homicides The z-test is simply explained in the linked article from the BMJ: https://www.bmj.com/content/332/7552/1256 It links us nicely to the next lesson, because it is aiming to test a specific hypothesis that the difference is zero… If you open the data file below, you can see I have calculated the z-test results for the difference between each season, year-on-year. ## # A tibble: 6 × 9 ## Season Games Goals GPG SE Lower95CI Upper95CI Change Z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-96 380 988 2.6 31.4 926. 1050. NA NA ## 2 1996-97 380 970 2.55 31.1 909. 1031. -18 -0.407 ## 3 1997-98 380 1019 2.68 31.9 956. 1082. 49 1.10 ## 4 1998-99 380 959 2.52 31.0 898. 1020. -60 -1.35 ## 5 1999-00 380 1060 2.79 32.6 996. 1124. 101 2.25 ## 6 2000-01 380 992 2.61 31.5 930. 1054. -68 -1.50 The ‘Z’ column is what we are interested in, and we are looking for a z-value greater than + or - 1.96 for a 95% test (analogous to the 95% intervals we’ve been dealing with so far). A simple way to visualize this is to plot the z-values for each season, and include ‘control lines’ which represent the + or -1.96 z value, beyond which we consider there to be a significant difference So, we can see that the 1999-2000 season, and the 2009-10 seasons exceed our z values, making them significantly different from the seasons before. Of course, we could do z-tests for any combination of two seasons, if we had a good reason. You can see on the ONS website they do this for different years’ murder rates to make a point. "],["ANOVA.html", "Chapter 8 ANOVA and Issues with Significance 8.1 Rate of Change in Football Goals per Season - Bonferroni Correction? 8.2 Statistical Power", " Chapter 8 ANOVA and Issues with Significance Here, I’ll demonstrate the basic application of ANOVA on the simple 3-group case of the Ed Sheeran Study: ## # A tibble: 6 × 3 ## ID GROUP ANGER ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 2 5 ## 3 3 1 2 ## 4 4 3 3 ## 5 5 2 4 ## 6 6 3 2 We need to tell R that GROUP is a factor variable not a numeric one: Check it worked: Table 8.1: Data summary Name ED Number of rows 45 Number of columns 3 _______________________ Column type frequency: factor 1 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts GROUP 0 1 FALSE 3 1: 15, 2: 15, 3: 15 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1 23.00 13.13 1 12 23 34 45 ▇▇▇▇▇ ANGER 0 1 3.22 1.17 1 2 3 4 5 ▁▇▇▆▅ Let’s create a quick table of the group means ## # A tibble: 3 × 5 ## GROUP variable n mean sd ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ANGER 15 2.33 0.816 ## 2 2 ANGER 15 4.27 0.704 ## 3 3 ANGER 15 3.07 1.03 And now visualize that in a boxplot: Hmmm…. Let’s run an ANOVA (we’re using the rstatix package here not Base R) Much of the following is drawn from: https://www.datanovia.com/en/lessons/anova-in-r/ ## ANOVA Table (type II tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 GROUP 2 42 19.235 1.17e-06 * 0.478 Results here suggest there is a significant effect (p-value is very small) rstatix also gives us an ‘effect size’ measure (ges, generalized eta-squared) which is also useful to us and suggests the effect is quite large. This can be interpreted similarly to a regression coefficient (which is also an effect size measure), and is the amount of variance in the dependent variable (Anger) that is explained by group membership. However, ANOVA only tests the ‘general effect’ of the treatment / group. We don’t know whether this is because of the difference between all of the groups, or only some. E.g., is it that there is an effect of music in general (i.e. between Control and Ed, and Control and Music, but not between Ed and Music), or that Ed specifically is anger-inducing (in whcih case we would see an effect between Ed and Music, and Ed and Control, and not between Music and Control). We can investigate this using Post-Hoc tests, which compare the individual groups. This has the potential for a multiple comparisons problem, which we will need to deal with. Let’s take a look back to the slide deck… ## # A tibble: 3 × 9 ## term group1 group2 null.value estimate conf.low conf.high p.adj ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GROUP 1 2 0 1.93 1.17 2.70 0.000000734 ## 2 GROUP 1 3 0 0.733 -0.0313 1.50 0.0625 ## 3 GROUP 2 3 0 -1.2 -1.96 -0.435 0.00126 ## # ℹ 1 more variable: p.adj.signif &lt;chr&gt; We can actually plot these results in a really effective way: This very clearly tells us that it is the Ed Sheeran group (2)that is driving these results, and there isn’t much to choose between the control group, and the ‘music’ group. Now, there are many other things that if we were doing ANOVA that we would also look to deal with - such as the various assumptions required of ANOVA, and so forth. But, they are beyond our scope in this class. Suffice to say that this has only scratched the surface of ANOVA so far. 8.1 Rate of Change in Football Goals per Season - Bonferroni Correction? Here, I’m using data from https://www.footballhistory.org/league/premier-league-statistics.html I hand-entered this into a spreadsheet, and calculated the additional stuff. ## # A tibble: 6 × 7 ## Season Games Goals GPG SE Lower95CI Upper95CI ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-96 380 988 2.6 31.4 926. 1050. ## 2 1996-97 380 970 2.55 31.1 909. 1031. ## 3 1997-98 380 1019 2.68 31.9 956. 1082. ## 4 1998-99 380 959 2.52 31.0 898. 1020. ## 5 1999-00 380 1060 2.79 32.6 996. 1124. ## 6 2000-01 380 992 2.61 31.5 930. 1054. You can see here I have calculated the standard errors from the yearly goal totals (which represent that year’s underlying rate of goal occurrence), then used that to calculate the 95% Confidence Interval limits We can use these to create a nifty chart with the error bars…drawing from the code used by Spiegelhalter in his book for Figure 9.4 available on his github (linked in the code). From this chart, and looking at the data itself, we can see that the 95% Intervals overlap, so it is hard to conclude that the underlying rate of goals has changed significantly year on year. Yes, even in the pandemic. Remember though, the ONS suggest that it is over-stringent to rely on error bar overlap, so we can also use z-tests to directly test the assumption that the change is zero. See: https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/compendium/focusonviolentcrimeandsexualoffences/yearendingmarch2016/homicide#statistical-interpretation-of-trends-in-homicides Rather than just use a z-value cutoff of 1.96 as we did last time, in the next data file, I have calculated the p-value (2 tailed as we do not hypothesize a direction for the difference) for the z-scores for the difference between each season, year-on-year. ## # A tibble: 6 × 11 ## Season Games Goals GPG SE Lower95CI Upper95CI Change Z negged ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-96 380 988 2.6 31.4 926. 1050. NA NA NA ## 2 1996-97 380 970 2.55 31.1 909. 1031. -18 -0.407 -0.407 ## 3 1997-98 380 1019 2.68 31.9 956. 1082. 49 1.10 -1.10 ## 4 1998-99 380 959 2.52 31.0 898. 1020. -60 -1.35 -1.35 ## 5 1999-00 380 1060 2.79 32.6 996. 1124. 101 2.25 -2.25 ## 6 2000-01 380 992 2.61 31.5 930. 1054. -68 -1.50 -1.50 ## # ℹ 1 more variable: p2 &lt;dbl&gt; Here, we can plot the p-values (2-tailed), and again we see (just like with the z-tests) that the same two seasons have significant differences. We can see that the 1999-2000 season, and the 2009-10 seasons have p values less than 0.05 The question is are we suffering from the multiple comparisons problem? Should we correct for it? It’s hard to say actually. Of course, we are indeed running multiple tests, 29 in fact. So, the chance of a false positive is quite high, if the null was true in all cases. The Bonferroni correction would immediately reduce this, but at the cost of making none of our tests significant. Further, the basis of these corrections is that the null hypothesis is true. What if it is the alternative hypothesis (that is, the H of an effect existing) that is true? In such cases, there can of course be no false positives. Here, you are increasing the chances of a false negative by reducing the chances of a false positive. So, what are the potential costs of each of these mistakes? For example, Thomas Perneger’s 1998 paper in the BMJ is scathing about the Bonferroni adjustment. Take a look at https://www.bmj.com/content/316/7139/1236.full Mind you, I am not saying that’s the final word, just that there are multiple perspectives on the issues! It’s never as simple as it seems when making statistical decisions, is it? 8.2 Statistical Power Different types of analysis and research design require different types of power calculation. In R, we can use the pwr package to calculate quite a few. Let’s calculate the required sample size for the Ed Sheeran study we conducted earlier. Really, we should have done this before collecting data, but it’s a nice example to do it post hoc. We don’t need the data, just the parameters of the experiment and analysis design. So, we had 3 groups, and used ANOVA Let’s set a significance of 0.05, a required power of 0.8, and assume the effect size is moderate (say 0.25) ## ## Balanced one-way analysis of variance power calculation ## ## k = 3 ## n = 52.3966 ## f = 0.25 ## sig.level = 0.05 ## power = 0.8 ## ## NOTE: n is number in each group So, we really wanted to have around 50 in each group to have an 80% chance of detecting a moderate effect presuming the null was true. You can see that my study (with only 15 in each group) was rather underpowered. However, if I had increased the effect size in the calculation to 0.5 (close to what the experiment suggested) this would have given me a result for n closer to what I actually used. However, you’d have to be VERY confident in the size of your likely effect to actually do that I think. "],["H-testing.html", "Chapter 9 Classical Statistical Hypothesis Testing 9.1 Premier League Goals and the Poisson Distribution 9.2 Correlation Significance Tests 9.3 Regression Significance Tests", " Chapter 9 Classical Statistical Hypothesis Testing This lesson contains some (re)worked examples from earlier sessions, focusing specifically on the hypothesis testing aspect of them. 9.1 Premier League Goals and the Poisson Distribution In this section, we are not really worried about the actual goals or anything like that. What we are concerned about is formally testing the hypothesis that Premier League goals per game actually do follow a Poisson distribution. To do this, we will use what is called a chi-square test, which uses the chi-square statistic. Chi-square is a fantastically versatile statistic, which is often used to compare tables of observations. Commonly, we compare a table of observed events with a table of events that we would expect if the null hypothesis were true in the population. The chi-square value is the measure of the difference between observed and expected counts in the table. We can use it here to compare the amount of goals we actually did observe in the 21-22 season, with those that we would expect to observe if the goals per game really do follow a Poisson distribution. Again, much of the idea and code for this section comes from: https://bookdown.org/theqdata/honors_thesis/goal-scoring-and-the-poisson-process.html The data comes from: https://www.football-data.co.uk/englandm.php and is the EPL results for 21-22 Season, with a new column of Total Goals (TOTG) created by me in Excel. ## # A tibble: 6 × 107 ## Div Date Time HomeTeam AwayTeam FTHG FTAG ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E0 2021-08-13 00:00:00 1899-12-31 20:00:00 Brentford Arsenal 2 0 ## 2 E0 2021-08-14 00:00:00 1899-12-31 12:30:00 Man United Leeds 5 1 ## 3 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Burnley Brighton 1 2 ## 4 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Chelsea Crystal … 3 0 ## 5 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Everton Southamp… 3 1 ## 6 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Leicester Wolves 1 0 ## # ℹ 100 more variables: TOTG &lt;dbl&gt;, FTR &lt;chr&gt;, HTHG &lt;dbl&gt;, HTAG &lt;dbl&gt;, ## # HTR &lt;chr&gt;, Referee &lt;chr&gt;, HS &lt;dbl&gt;, AS &lt;dbl&gt;, HST &lt;dbl&gt;, AST &lt;dbl&gt;, ## # HF &lt;dbl&gt;, AF &lt;dbl&gt;, HC &lt;dbl&gt;, AC &lt;dbl&gt;, HY &lt;dbl&gt;, AY &lt;dbl&gt;, HR &lt;dbl&gt;, ## # AR &lt;dbl&gt;, B365H &lt;dbl&gt;, B365D &lt;dbl&gt;, B365A &lt;dbl&gt;, BWH &lt;dbl&gt;, BWD &lt;dbl&gt;, ## # BWA &lt;dbl&gt;, IWH &lt;dbl&gt;, IWD &lt;dbl&gt;, IWA &lt;dbl&gt;, PSH &lt;dbl&gt;, PSD &lt;dbl&gt;, ## # PSA &lt;dbl&gt;, WHH &lt;dbl&gt;, WHD &lt;dbl&gt;, WHA &lt;dbl&gt;, VCH &lt;dbl&gt;, VCD &lt;dbl&gt;, ## # VCA &lt;dbl&gt;, MaxH &lt;dbl&gt;, MaxD &lt;dbl&gt;, MaxA &lt;dbl&gt;, AvgH &lt;dbl&gt;, AvgD &lt;dbl&gt;, … ## Length Class Mode ## 380 character character The above is a quick way of calculating / checking the total number of games if you don’t know already. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 3.000 2.818 4.000 9.000 ## [1] 1071 First, let’s create a table of all the matches with different numbers of goals, just as we did in Lesson 6. ## # A tibble: 9 × 2 ## TOTG ActualMatches ## &lt;dbl&gt; &lt;int&gt; ## 1 0 22 ## 2 1 65 ## 3 2 88 ## 4 3 81 ## 5 4 60 ## 6 5 43 ## 7 6 17 ## 8 7 3 ## 9 9 1 Here, we have a problem. The chi-square test has a condition that the expected values in every column will be greater than 5, and as we go down the Poisson probabilities (i.e. for higher total goals), it is clear that this will start to become a problem for us. The solution is simple, we combine the numbers for larger numbers of goals into a single ‘x and above’ variable. Here, let’s do it for 6 goals and above. To do so, we first need to create a new table, where the values for 6 and above are combined into a new row. ## # A tibble: 7 × 2 ## TOTG ActualMatches ## &lt;chr&gt; &lt;int&gt; ## 1 0 22 ## 2 1 65 ## 3 2 88 ## 4 3 81 ## 5 4 60 ## 6 5 43 ## 7 6 or more 21 Now we can create the Poisson distribution for the relevant mean, which means we first need to create the stats… ## min Q1 median Q3 max mean sd n missing ## 0 2 3 4 9 2.818421 1.626359 380 0 Below is some code which will help us to create the distributions and also check them. ## [1] 2.818421 ## [1] 2.645042 As we saw in Lesson 7, while for a Poisson distribution these should be exactly the same, we decided that they were good enough to be going on with. So, let’s actually check this formally with the Chi-square test. We do so by first creating a table of the Poisson probabilities for x or fewer goals, for the Poisson with a mean of 2.818 (i.e. the variable MeanGoals) ## PoisProb ## 1 0.05970013 ## 2 0.16826011 ## 3 0.23711392 ## 4 0.22276228 ## 5 0.15695948 ## 6 0.08847558 ## 7 0.04156024 This allows us to make some predictions of what we could expect. Remembering again that there are 380 games in a season, we can see that there is a 0.16% chance of seeing 4 goals in a game, which equates to 380 x 0.16 = 60.8 games we would expect to see in a season with 4 goals. We can check this out by creating a new table comparing actual with predicted values… ## TOTG ActualMatches PoisProb ExpectedMatches ## 1 0 22 0.05970013 23 ## 2 1 65 0.16826011 64 ## 3 2 88 0.23711392 90 ## 4 3 81 0.22276228 85 ## 5 4 60 0.15695948 60 ## 6 5 43 0.08847558 34 ## 7 6 or more 21 0.04156024 16 This looks quite frighteningly close up until you get to the 5 row. It will be interesting to see how we go with a formal chi-square test. ## ## Chi-squared test for given probabilities ## ## data: NewGoalsTable$ActualMatches ## X-squared = 4.2244, df = 6, p-value = 0.6463 The P-value here is actually 0.64. This suggests that there is not a significant difference between the distribution of the actual goals, and that which would be expected if they did follow a Poisson distribution. 9.2 Correlation Significance Tests Here, we revisit our correlation between GDP per capita, and Happiness metrics, which I pulled from Our Word in Data: ## Country Happiness GDPpc Pop ## Length:249 Min. :2.400 Min. : 731 Min. :8.090e+02 ## Class :character 1st Qu.:4.670 1st Qu.: 4917 1st Qu.:4.153e+05 ## Mode :character Median :5.530 Median : 12655 Median :5.596e+06 ## Mean :5.492 Mean : 20464 Mean :5.918e+07 ## 3rd Qu.:6.260 3rd Qu.: 30100 3rd Qu.:2.421e+07 ## Max. :7.820 Max. :112557 Max. :4.663e+09 ## NA&#39;s :96 NA&#39;s :52 NA&#39;s :7 ## # A tibble: 6 × 4 ## Country Happiness GDPpc Pop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2.4 1971 38972236 ## 2 Albania 5.2 13192 2866850 ## 3 Algeria 5.12 10735 43451668 ## 4 American Samoa NA NA 46216 ## 5 Andorra NA NA 77723 ## 6 Angola NA 6110 33428490 Let’s not worry about plotting the data, and go straight to the correlation: ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 13.502, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6636288 0.8092331 ## sample estimates: ## cor ## 0.745184 Here are our results. The estimate is a correlation, and we test that using the t statistic. The t-value is simply the estimate divided by the standard error (which we can’t see in this output), and is interpreted essentially as ‘how far from 0 is the estimate, in standard errors’. The p-value for t is very very small, and obviously less than 0.05. 9.3 Regression Significance Tests The process to asses the significance of regression estimates is very very similar to that for correlations. Let’s revisit the heart disease data from: https://www.scribbr.com/statistics/linear-regression-in-r/ ## ...1 biking smoking heart.disease ## Min. : 1.0 Min. : 1.119 Min. : 0.5259 Min. : 0.5519 ## 1st Qu.:125.2 1st Qu.:20.205 1st Qu.: 8.2798 1st Qu.: 6.5137 ## Median :249.5 Median :35.824 Median :15.8146 Median :10.3853 ## Mean :249.5 Mean :37.788 Mean :15.4350 Mean :10.1745 ## 3rd Qu.:373.8 3rd Qu.:57.853 3rd Qu.:22.5689 3rd Qu.:13.7240 ## Max. :498.0 Max. :74.907 Max. :29.9467 Max. :20.4535 ## # A tibble: 6 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.8 10.9 11.8 ## 2 2 65.1 2.22 2.85 ## 3 3 1.96 17.6 17.2 ## 4 4 44.8 2.80 6.82 ## 5 5 69.4 16.0 4.06 ## 6 6 54.4 29.3 9.55 Let’s go straight the the multiple regression model. ## ## Call: ## lm(formula = heart.disease ~ biking + smoking, data = Heart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1789 -0.4463 0.0362 0.4422 1.9331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.984658 0.080137 186.99 &lt;2e-16 *** ## biking -0.200133 0.001366 -146.53 &lt;2e-16 *** ## smoking 0.178334 0.003539 50.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.654 on 495 degrees of freedom ## Multiple R-squared: 0.9796, Adjusted R-squared: 0.9795 ## F-statistic: 1.19e+04 on 2 and 495 DF, p-value: &lt; 2.2e-16 We interpret these just as we did the correlation significance tests. The t-value is large, and the p-value (two-tailed) is small. Interestingly, R gives ‘stars’ for the different levels of significance, so to some extent they are doing some decision making for you. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
